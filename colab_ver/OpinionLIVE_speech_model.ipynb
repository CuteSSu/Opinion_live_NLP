{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OpinionLIVE_speech_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"7-T5qDpMpWT7"},"source":["##드라이브 마운트"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9ojbYwHpWDZ","executionInfo":{"status":"ok","timestamp":1630219814839,"user_tz":-540,"elapsed":20444,"user":{"displayName":"임수희","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgrCboa0yULlM0dWSpMxDBW0DKqmTq3cpXTIP8b=s64","userId":"13082555440283330047"}},"outputId":"48c6e9b9-15ad-4c86-b224-c1d13b806559"},"source":["# 구글 드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7LZzkONppb5K"},"source":["## package 설치"]},{"cell_type":"code","metadata":{"id":"PLL-BDCfpbiZ"},"source":["try:\n","    import transformers, emoji, soynlp, pytorch_lightning\n","except:\n","    !pip install -U -q transformers emoji soynlp pytorch-lightning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDixdsO2peKa"},"source":["!pip3 install adamp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3N4epJmpgzu"},"source":["### 패키지 import & 기본 Args 설정"]},{"cell_type":"code","metadata":{"id":"nnGKfhdSphwU"},"source":["import os \n","import pandas as pd\n","\n","from pprint import pprint\n","\n","# 텐서 등의 다양한 수학 함수가 포함되어져 있으며 Numpy와 유사한 구조\n","import torch\n","#  torch.utils.data: SGD의 반복 연산을 실행할 때 사용하는 미니 배치용 유틸리티 함수가 포함되어져 있습니다.\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","# lr_scheduler: 학습과정에서 learning rate를 조정\n","# torch.optim: 확률적 경사하강법을 중심으로 한 파라미터 최적화 알고리즘이 구현\n","# ExponentialLR: 매 epoch마다 이전 lr에 gamma만큼 곱해서 사용\n","from torch.optim.lr_scheduler import ExponentialLR\n","\n","# pytorch_lightning: 정돈된 코드 스타일을 갖추기 위해 사용\n","# LightningModule: 모델 내부의 구조를 설계하는 research & science 클래스, 모델의 구조나 데이터 전처리, 손실함수 등의 설정 등을 통해 모델을 초기화\n","from pytorch_lightning import LightningModule, Trainer, seed_everything\n","\n","from transformers import ElectraForSequenceClassification, AutoTokenizer, AdamW\n","# from adamp import AdamP\n","# precision_score: 양성 클래스에 속한다고 출력한 샘플 중 실제로 양성 클래스에 속하는 샘플 수의 비율, 높을수록 좋은 모형\n","# recall_score: 실제 양성 클래스에 속한 표본 중에 양성 클래스에 속한다고 출력한 표본의 수의 비율, 높을수록 좋은 모형\n","# f1_score: 정밀도와 재현율의 가중조화평균\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","import re\n","import emoji\n","from soynlp.normalizer import repeat_normalize"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iNwrZMLlpkN6"},"source":["### 기본 학습 Arguments"]},{"cell_type":"code","metadata":{"id":"vHSZRme5plI2"},"source":["args = {\n","    'random_seed':42, # Random Seed, 동일한 난수를 발생시키기 위해\n","    'pretrained_model': 'beomi/KcELECTRA-base',  # Transformers PLM name\n","    'pretrained_tokenizer': '',  # Optional, Transformers Tokenizer Name. Overrides `pretrained_model`\n","    'batch_size': 64,\n","    'lr': 2e-5,  # Starting Learning Rate\n","    'epochs': 6,  # Max Epochs\n","    'max_length': 32,  # Max Length input size\n","    'train_data_path': \"/content/drive/MyDrive/opinionlive/data/0824_train_80%.xlsx\",  # Train Dataset file \n","    'val_data_path': \"/content/drive/MyDrive/opinionlive/data/0824_test_20%.xlsx\",  # Validation Dataset file \n","    'test_mode': False,  # Test Mode enables `fast_dev_run`\n","    'optimizer': 'AdamP',  # AdamW vs AdamP\n","    'lr_scheduler': 'exp',  # ExponentialLR vs CosineAnnealingWarmRestarts\n","    'fp16': True,  # Enable train on FP16(if GPU)\n","    'tpu_cores': 0,  # Enable TPU with 1 core or 8 cores\n","    'cpu_workers': os.cpu_count(),\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PEOZtpnmpnq2"},"source":["## Pytorch Lightning으로 모델 만들기"]},{"cell_type":"code","metadata":{"id":"4wzMwWa-poEx"},"source":["acc_test = pd.DataFrame(columns=['train', 'test'])\n","loss_test = pd.DataFrame(columns=['train', 'test'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTGki8b8psnQ"},"source":["train_acc = []\n","train_loss = []\n","test_acc = []\n","test_loss = []\n","a = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNmSex8FptGu"},"source":["# 모델 클래스\n","class Model(LightningModule):\n","    def __init__(self, **kwargs):   #kwargs: keyword argument의 줄임말로 키워드를 제공, 모든 값을 모델 입력에 직접 넣어주는 것은 상당히 귀찮기 때문에 사용 \n","        super().__init__() #__init__ 클래스를 선언하는 순간  실행되는 함수\n","        self.save_hyperparameters() # 이 부분에서 self.hparams에 위 kwargs가 저장된다.\n","        \n","        self.clsfier = ElectraForSequenceClassification.from_pretrained(self.hparams.pretrained_model,num_labels=8)\n","        self.tokenizer = AutoTokenizer.from_pretrained(\n","            self.hparams.pretrained_tokenizer\n","            if self.hparams.pretrained_tokenizer\n","            else self.hparams.pretrained_model\n","        )\n","    # forward: foward() 함수는 모델이 학습데이터를 입력받아서 forward 연산을 진행시키는 함수\n","    def forward(self, **kwargs):\n","        return self.clsfier(**kwargs)\n","\n","    # step 함수: 활성화 함수 중 하나\n","    def step(self, batch, batch_idx):\n","        data, labels = batch\n","        output = self(input_ids=data, labels=labels)\n","\n","        # Transformers 4.0.0+ 트랜스포머 자체를 좀 공부해야함\n","        \n","        loss = output.loss\n","        logits = output.logits\n","\n","        # dim은 차원을 설정해주는 dim= -1 마지막 차원을 제거한다는 의미 2차원이면 열의 차원을 제거한다는 의미\n","        preds = logits.argmax(dim=-1) \n","\n","        #tensor값만 뽑기 위해 labes.cpu().numpy()를 하는 것\n","        y_true = list(labels.cpu().numpy())  \n","        y_pred = list(preds.cpu().numpy())\n","\n","\n","        return {\n","            'loss': loss,\n","            'y_true': y_true,\n","            'y_pred': y_pred,\n","        }\n","    #모델 학습 루프(Training, Validation, Test Loop)\n","\n","    def training_step(self, batch, batch_idx):\n","        return self.step(batch, batch_idx) \n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.step(batch, batch_idx)\n","\n","      #  1 epoch 종료\n","    def epoch_end(self, outputs, state='train'):\n","        loss = torch.tensor(0, dtype=torch.float)\n","        for i in outputs:\n","            loss += i['loss'].cpu().detach()\n","        loss = loss / len(outputs)\n","\n","        y_true = []\n","        y_pred = []\n","        for i in outputs:\n","            y_true += i['y_true']\n","            y_pred += i['y_pred']\n","        \n","        acc = accuracy_score(y_true, y_pred)\n","        # average = 'micro' 추가\n","        prec = precision_score(y_true, y_pred,average='micro')\n","        rec = recall_score(y_true, y_pred,average='micro')\n","        f1 = f1_score(y_true, y_pred,average='micro')\n","\n","        self.log(state+'_loss', float(loss), on_epoch=True, prog_bar=True)\n","        self.log(state+'_acc', acc, on_epoch=True, prog_bar=True)\n","        self.log(state+'_precision', prec, on_epoch=True, prog_bar=True)\n","        self.log(state+'_recall', rec, on_epoch=True, prog_bar=True)\n","        self.log(state+'_f1', f1, on_epoch=True, prog_bar=True)\n","        \n","        # global: 전역변수 설정 그래프를 그리기 위해\n","        global a\n","        if a == 0 : \n","          test_acc.append(acc)\n","          test_loss.append(loss)\n","          a = 1\n","        elif a == 1 :\n","          train_acc.append(acc)\n","          train_loss.append(loss)\n","          a = 0\n","          \n","        print(f'[Epoch {self.trainer.current_epoch} {state.upper()}] Loss: {loss}, Acc: {acc}, Prec: {prec}, Rec: {rec}, F1: {f1}')\n","        return {'loss': loss}\n","    \n","    def training_epoch_end(self, outputs):\n","        self.epoch_end(outputs, state='train')\n","\n","    def validation_epoch_end(self, outputs):\n","        self.epoch_end(outputs, state='val')\n","    \n","    #optimizer 설정\n","    def configure_optimizers(self):\n","        if self.hparams.optimizer == 'AdamW':\n","            optimizer = AdamW(self.parameters(), lr=self.hparams.lr)\n","        elif self.hparams.optimizer == 'AdamP':\n","            from adamp import AdamP\n","            optimizer = AdamP(self.parameters(), lr=self.hparams.lr)\n","        else:\n","            raise NotImplementedError('Only AdamW and AdamP is Supported!')\n","        if self.hparams.lr_scheduler == 'cos':\n","            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n","        elif self.hparams.lr_scheduler == 'exp':\n","            scheduler = ExponentialLR(optimizer, gamma=0.5)\n","        else:\n","            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n","        return {\n","            'optimizer': optimizer,\n","            'scheduler': scheduler,\n","        }\n","\n","    #파일 형식 \n","    def read_data(self, path):\n","        if path.endswith('xlsx'):\n","            return pd.read_excel(path)\n","        elif path.endswith('csv'):\n","            return pd.read_csv(path)\n","        elif path.endswith('tsv') or path.endswith('txt'):\n","            return pd.read_csv(path, sep='\\t')\n","        else:\n","            raise NotImplementedError('Only Excel(xlsx)/Csv/Tsv(txt) are Supported')\n","    \n","    def clean(self, x):\n","        emojis = ''.join(emoji.UNICODE_EMOJI.keys())\n","        pattern = re.compile(f'[^ .,?!/@$%~％·∼()\\x00-\\x7Fㄱ-힣{emojis}]+')\n","        url_pattern = re.compile(\n","            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)')\n","        x = pattern.sub(' ', x)\n","        x = url_pattern.sub('', x)\n","        x = x.strip()\n","        x = repeat_normalize(x, num_repeats=2)\n","        return x\n","\n","    def encode(self, x, **kwargs):\n","        return self.tokenizer.encode(\n","            self.clean(str(x)),\n","            padding='max_length',\n","            max_length=self.hparams.max_length,\n","            truncation=True,\n","            **kwargs,\n","        )\n","\n","    def preprocess_dataframe(self, df):\n","        df['command'] = df['command'].map(self.encode)\n","        return df\n","\n","    # shuffle=False: 순차적 분할\n","    def dataloader(self, path, shuffle=False):\n","        df = self.read_data(path)\n","        df = self.preprocess_dataframe(df)\n","\n","        # TensorDataset은 Dataset을 상속한 클래스로 학습 데이터 X와 레이블 Y를 묶어 놓는 컨테이너\n","        # TensorDataset을 DataLoader에 전달하면 for 루프에서 데이터의 일부분만 간단히 추출\n","        dataset = TensorDataset(\n","            torch.tensor(df['command'].to_list(), dtype=torch.long),\n","            torch.tensor(df['label'].to_list(), dtype=torch.long),\n","        )\n","\n","        return DataLoader(\n","            dataset,\n","            batch_size=self.hparams.batch_size * 1 if not self.hparams.tpu_cores else self.hparams.tpu_cores,\n","            shuffle=shuffle,\n","            num_workers=self.hparams.cpu_workers,\n","        )\n","\n","    def train_dataloader(self):\n","        return self.dataloader(self.hparams.train_data_path, shuffle=True)\n","\n","    def val_dataloader(self):\n","        return self.dataloader(self.hparams.val_data_path, shuffle=False)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wtqBFHjbpwbZ"},"source":["from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","checkpoint_callback = ModelCheckpoint(\n","    filename='epoch{epoch}-val_acc{val_acc:.4f}',\n","    # monitor: 어떤 metric을 기준으로 체크포인트를 저장할지 지정\n","    monitor='val_acc',\n","    # save_top_k: 최대 몇 개의 체크포인트를 저장할지 지정, save_last에 의해 저장되는 체크포인트는 제외 \n","    save_top_k=3,\n","    # mode: 지정한 metric의 어떤 기준(min, max)으로 체크포인트를 저장할지 지정\n","    mode='max',\n","    auto_insert_metric_name=False,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IQeyX5YIpy8C"},"source":["## 학습하기"]},{"cell_type":"code","metadata":{"id":"4wpdfebLpz04"},"source":["print(\"Using PyTorch Ver\", torch.__version__)\n","print(\"Fix Seed:\", args['random_seed'])\n","seed_everything(args['random_seed'])\n","model = Model(**args)\n","\n","print(\":: Start Training ::\")\n","trainer = Trainer(\n","    callbacks=[checkpoint_callback],\n","    max_epochs=args['epochs'],\n","    fast_dev_run=args['test_mode'],\n","    num_sanity_val_steps=None if args['test_mode'] else 0,\n","    # For GPU Setup\n","    deterministic=torch.cuda.is_available(),\n","    gpus=[0] if torch.cuda.is_available() else None,  # 0번 idx GPU  사용\n","    precision=16 if args['fp16'] and torch.cuda.is_available() else 32,\n","    # For TPU Setup\n","    # tpu_cores=args['tpu_cores'] if args['tpu_cores'] else None,\n",")\n","trainer.fit(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a1hIFIZ2p37N"},"source":["### accuracy, loss값 그래프 시각화"]},{"cell_type":"code","metadata":{"id":"b1xBVS1yp5El"},"source":["for i in range(args['epochs']):\n","  acc_test.loc[i, 'train'] = test_acc[i]\n","  acc_test.loc[i, 'test'] = train_acc[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3CgIhNhJp6V2"},"source":["for i in range(args['epochs']):\n","  loss_test.loc[i, 'train'] = float(test_loss[i].numpy())\n","  loss_test.loc[i, 'test'] = float(train_loss[i].numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"86-BVXMhp6nJ"},"source":["import matplotlib.pyplot as plt\n","acc_test.plot()\n","plt.ylim([0.0, 1.1])     # Y축의 범위: [ymin, ymax]\n","# plt.title(\"정확도\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"acc\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V9k15nOTp9Hw"},"source":["import matplotlib.pyplot as plt\n","loss_test.plot()\n","plt.ylim([0.0, 2.1])     # Y축의 범위: [ymin, ymax]\n","# plt.title(\"정확도\")\n","plt.xlabel(\"epochs\")\n","plt.ylabel(\"loss\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"svWfG1D7p-kZ"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"ZUM5fua8p_VV"},"source":["from glob import glob\n","sorted(glob('./lightning_logs/version_0/checkpoints/*.ckpt'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AFn07Zt-qCf2"},"source":["from glob import glob\n","\n","latest_ckpt = sorted(glob('./lightning_logs/version_0/checkpoints/*.ckpt'))[-1]\n","latest_ckpt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bnGkNSTzqDCv"},"source":["model = Model.load_from_checkpoint(latest_ckpt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikQfmkTAqKJg"},"source":["model = joblib.load('/content/drive/MyDrive/opinionlive/model_pkl/KoElectraModel.pkl')\n","\n","def infer(x):\n","    return torch.softmax(\n","        model(**model.tokenizer(x, return_tensors='pt')\n","    ).logits, dim=-1), print(\"          1:명령, 2:약속, 3:정표, 4:기대, 5:주장, 6:갈등, 7:진술, 8:질문\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRtdOLkyqLEF"},"source":["infer(\"사랑스러운 수미니\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jDJjGdibSBIX"},"source":["##모델저장"]},{"cell_type":"code","metadata":{"id":"O05bNbODqEi6"},"source":["import joblib\n","joblib.dump(model, '/content/drive/MyDrive/opinionlive/model_pkl/kcElectraModel.pkl')"],"execution_count":null,"outputs":[]}]}